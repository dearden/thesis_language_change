{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter fp of json with tokens:\n",
      "C:/Users/Eddie/Documents/Datasets/Hansard Output/Thesis_ACE/tokens.json\n",
      "Enter a directory to put results in:\n",
      "C:/Users/Eddie/Documents/Datasets/Hansard Output/Thesis_ACE/PoS_Chunks\n",
      "Data ready:  0:02:16.331741\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import nltk\n",
    "import regex as re\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "from helper_functions import tokenise, get_contribution_windows, split_corpus, check_dir, make_tok_chunks, get_chunks\n",
    "from mp_sampling import multi_mp_splits_with_limit, multi_mp_splits, get_end_of_windows\n",
    "from run_CE_experiments import get_groups_toks_and_contribs\n",
    "\n",
    "with open(\"../project-config.json\") as config_file:\n",
    "    project_config = json.load(config_file)\n",
    "\n",
    "DB_FP = project_config[\"DB_FP\"]\n",
    "MP_Group_FP = project_config[\"GROUPS_FP\"]\n",
    "with open(project_config[\"SPEAKER_FILE\"]) as speaker_file:\n",
    "    speaker_list = json.load(speaker_file)\n",
    "\n",
    "sql_get_all_posts =\"\"\"\n",
    "SELECT c.uid, m.name, m.PimsId, p.party, d.date, c.body, c.topic, c.section, s.tmay_deal, s.benn_act, s.ref_stance, s.constituency_leave\n",
    "FROM contributions as c\n",
    "INNER JOIN members as m\n",
    "ON m.PimsId = c.member\n",
    "INNER JOIN debates as d\n",
    "ON d.uid = c.debate\n",
    "INNER JOIN member_party as p\n",
    "ON p.PimsId = m.PimsId\n",
    "INNER JOIN member_stances as s\n",
    "ON s.PimsId = m.PimsId\n",
    "WHERE (d.date BETWEEN date(\"2015-05-01\") AND date(\"2019-12-11\"))\n",
    "AND (((d.date BETWEEN p.start AND p.end) AND NOT (p.end IS NULL))\n",
    "OR ((d.date >= p.start) AND (p.end IS NULL)));\"\"\".strip()\n",
    "\n",
    "\n",
    "def single_CE_run(gnames, curr_contribs, curr_toks, curr_ref, curr_ref_toks,\n",
    "                    win_size, win_step, n_runs, balanced_groups, w_limit,\n",
    "                    token_limit, n_contribs_per_mp, out_fp):\n",
    "\n",
    "    if w_limit:\n",
    "        # For doing with a limit per MP\n",
    "        comparisons, meta = multi_mp_splits_with_limit(gnames,\n",
    "                                                        list(curr_contribs.values()),\n",
    "                                                        list(curr_toks.values()),\n",
    "                                                        curr_ref, curr_ref_toks,\n",
    "                                                        window_func=get_contribution_windows,\n",
    "                                                        window_size=win_size, window_step=win_step,\n",
    "                                                        n_runs=n_runs, balanced_groups=balanced_groups,\n",
    "                                                        comp_method=\"CE\", n_words_per_contribution=token_limit,\n",
    "                                                        n_contribs_per_mp=n_contribs_per_mp)\n",
    "    else:\n",
    "        comparisons, meta = multi_mp_splits(gnames,\n",
    "                                            list(curr_contribs.values()),\n",
    "                                            list(curr_toks.values()),\n",
    "                                            curr_ref, curr_ref_toks,\n",
    "                                            window_func=get_contribution_windows,\n",
    "                                            window_size=win_size, window_step=win_step,\n",
    "                                            n_runs=n_runs, balanced_groups=balanced_groups,\n",
    "                                            comp_method=\"CE\", n_words_per_contribution=token_limit)\n",
    "\n",
    "    end_of_windows = get_end_of_windows(pd.concat(list(curr_contribs.values()) + [curr_ref], axis=0),\n",
    "                                                    get_contribution_windows, win_size, win_step)\n",
    "    end_of_windows = [datetime.strftime(d, \"%Y-%m-%d\") for d in end_of_windows]\n",
    "\n",
    "    comparisons_dict = [{gsnap: {gtest: {datetime.strftime(w, \"%Y-%m-%d\"): run[gsnap][gtest][w].to_dict() for w in run[gsnap][gtest]} for gtest in run[gsnap]} for gsnap in run} for run in comparisons]\n",
    "\n",
    "    meta_dict = [{metaVal: {gname: {datetime.strftime(w, \"%Y-%m-%d\"): run[metaVal][gname][w] for w in run[metaVal][gname]} for gname in run[metaVal]} for metaVal in run} for run in meta]\n",
    "\n",
    "    param_combo = {\"win_type\": \"contributions\", \"win_size\": win_size, \"win_step\": win_step,\n",
    "                    \"n_runs\": n_runs, \"balanced\": balanced_groups, \"comp_method\": \"CE\",\n",
    "                    \"contrib_limit\": w_limit, \"token_limit\": token_limit, \"queries\": queries, \"gnames\": gnames}\n",
    "\n",
    "    out_dict = {\"params\": param_combo, \"comparisons\": comparisons_dict, \"meta\": meta_dict, \"end_of_windows\": end_of_windows}\n",
    "\n",
    "    with open(out_fp, \"w\") as out_file:\n",
    "        json.dump(out_dict, out_file)\n",
    "\n",
    "    print(\"Written file: \", out_fp)\n",
    "\n",
    "\n",
    "def read_tok_file(toks_fp):\n",
    "    with open(toks_fp, encoding=\"utf-8\") as tok_file:\n",
    "        for line in tok_file.readlines():\n",
    "            curr = json.loads(line.strip())\n",
    "            yield curr\n",
    "\n",
    "\n",
    "def load_pos(toks_fp):\n",
    "    for curr in read_tok_file(toks_fp):\n",
    "        yield curr[0], [w[\"pos\"] for w in curr[1]]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    tok_fp = input(\"Enter fp of json with tokens:\\n\")\n",
    "    out_dir = input(\"Enter a directory to put results in:\\n\")\n",
    "\n",
    "    check_dir(out_dir)\n",
    "\n",
    "    conn = sqlite3.connect(DB_FP)\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    # Gets all the contributions and creates a nice dataframe\n",
    "    all_contributions = pd.read_sql_query(sql_get_all_posts, conn)\n",
    "    all_contributions.columns = ['uid', 'name', 'PimsId', 'party', 'date', 'text', 'topic', 'section', 'tmay_deal', 'benn_act', 'ref_stance', 'constituency_leave']\n",
    "    all_contributions.set_index(\"uid\", inplace=True)\n",
    "    convert_to_date = lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    all_contributions['date'] = all_contributions['date'].apply(convert_to_date)\n",
    "\n",
    "    all_contributions = all_contributions.query(\"PimsId not in @speaker_list\")\n",
    "    all_contributions.sort_values(\"date\", inplace=True)\n",
    "\n",
    "    # Tokenise the contributions\n",
    "    all_toks = {x[0]: x[1] for x in load_pos(tok_fp)}\n",
    "    all_toks = pd.Series(all_toks)\n",
    "    all_toks = all_toks.loc[all_contributions.index]\n",
    "\n",
    "    # Get the EU and Non-EU mentions\n",
    "    eu_mentions, non_eu_mentions = split_corpus(all_contributions, \"eu\")\n",
    "\n",
    "    # Default parameters: 60k for all, 12k for EU.\n",
    "\n",
    "    # Setting the hyperparameters for all runs. Could adjust for each RQ if I wanted\n",
    "    win_size = 60000    # Number of contributions in each window\n",
    "    win_step = 60000    # Number of contribution each window moves on by.\n",
    "    n_runs = 50          # Number of runs. (50 takes quite a long time)\n",
    "    balanced=True       # Whether or not to balance the samples (same number of members in each group)\n",
    "    w_limit = True      # Whether or not to limit the number of contributions from each member\n",
    "    contrib_limit = 60  # Max number of contributions per member.\n",
    "    curr_contributions = all_contributions.drop(\"text\", axis=1)  # the contributions to use (either \"all_contributions\" or \"eu_mentions\")\n",
    "\n",
    "    # Keep only the tokens in the current contributions\n",
    "    all_toks = all_toks.loc[curr_contributions.index]\n",
    "\n",
    "    # Convert to chunks\n",
    "    chunk_size = 60\n",
    "    all_toks =  make_tok_chunks(all_toks, chunk_size) # Makes the chunks (with a new index)\n",
    "    idx_map = all_toks[\"idx\"]\n",
    "    # Gets contributions for chunks (and reindexes)\n",
    "    curr_contributions = curr_contributions.loc[idx_map.loc[idx_map.isin(curr_contributions.index)]].set_index(all_toks.index)\n",
    "    curr_contributions[\"speech_id\"] = idx_map\n",
    "    # eu_mentions = curr_contributions[idx_map.isin(eu_mentions.index)]\n",
    "    all_toks = all_toks[\"chunk\"]\n",
    "\n",
    "    print(\"Data ready: \", datetime.now() - startTime)\n",
    "    startTime = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015/05/18 66.0\n",
      "2015/10/26 65.5\n",
      "2016/02/05 69.5\n",
      "2016/06/08 68.0\n",
      "2016/11/22 76.5\n",
      "2017/03/08 55.0\n",
      "2017/10/24 68.0\n",
      "2018/02/05 69.5\n",
      "2018/06/06 69.5\n",
      "2018/11/12 66.5\n",
      "2019/02/25 68.0\n"
     ]
    }
   ],
   "source": [
    "for w, contribs in get_contribution_windows(curr_contributions, 60000, 60000):\n",
    "    print(w, contribs.groupby(\"PimsId\").size().median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hansard",
   "language": "python",
   "name": "hansard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
